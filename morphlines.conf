# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# "License"); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.

# Application configuration file in HOCON format (Human-Optimized Config Object Notation).
# HOCON syntax is defined at http://github.com/typesafehub/config/blob/master/HOCON.md
# and also used by Akka (http://www.akka.io) and Play (http://www.playframework.org/).
# For more examples see http://doc.akka.io/docs/akka/2.1.2/general/configuration.html

# morphline.conf example file
# this is a comment

# Specify server locations in a SOLR_LOCATOR variable; used later in variable substitutions:
SOLR_LOCATOR : {
  # Name of solr collection
  collection : collection1

  # ZooKeeper ensemble
  zkHost : "$ZK_HOST"

  # Relative or absolute path to a directory containing conf/solrconfig.xml and conf/schema.xml
  # If this path is uncommented it takes precedence over the configuration stored in ZooKeeper.
  # solrHomeDir : "example/solr/collection1"

  # The maximum number of documents to send to Solr per network batch (throughput knob)
  # batchSize : 100
}

# Specify an array of one or more morphlines, each of which defines an ETL
# transformation chain. A morphline consists of one or more (potentially
# nested) commands. A morphline is a way to consume records (e.g. Flume events,
# HDFS files or blocks), turn them into a stream of records, and pipe the stream
# of records through a set of easily configurable transformations on it's way to
# Solr (or a MapReduceIndexerTool RecordWriter that feeds via a Reducer into Solr).
morphlines : [
  {
    # Name used to identify a morphline. E.g. used if there are multiple morphlines in a
    # morphline config file
    id : morphline1

    # Import all morphline commands in these java packages and their subpackages.
    # Other commands that may be present on the classpath are not visible to this morphline.
    importCommands : ["org.kitesdk.**", "org.apache.solr.**"]

    commands : [
      {
        readCSV {
          separator: " "
          columns: [client_ip, ignored_1, ignored_2, time, ignored_timezone, request, code, bytes, referer, user_agent]
          ignoreFirstLine: false
          quoteChar: "\""
          commentPrefix: ""
          trim: true
          charset: UTF-8
        }
      }
      {
        split {
          inputField: request
          outputFields: [method, ignored_url, protocol]
          separator: " "
          isRegex: false
          addEmptyStrings: false
          trim: true
        }
      }
      #{
      #  split {
      #    inputField: url
      #    outputFields: ["", app, subapp]
      #    separator: "\/"
      #    isRegex: false
      #    separator : """\s*,\s*"""
      #    isRegex : true
      #    addEmptyStrings: false
      #    trim: true
      #  }
      #}
      {
        userAgent {
          inputField: user_agent
          outputFields: {
            user_agent_family: "@{ua_family}"
            user_agent_major: "@{ua_major}"
            device_family: "@{device_family}"
            os_family: "@{os_family}"
            os_major: "@{os_major}"
          }
        }
      }
      {
        #Extract GEO information
        geoIP {
          inputField: client_ip
          database: "/opt/GeoLite2-City_20171205/GeoLite2-City.mmdb"
        }
      }
      {

        # extract parts of the geolocation info from the Jackson JsonNode Java
        # # object contained in the _attachment_body field and store the parts in
        # # the given record output fields:
        extractJsonPaths {
          flatten: false
          paths: {
            country_code: /country/iso_code
            country_name: /country/names/en
            region_code: /continent/code
            city: /city/names/en
            latitude: /location/latitude
            longitude: /location/longitude
          }
        }
      }
      #{logInfo { format : "BODY : {}", args : ["@{}"] } }
      # add Unique ID, in case our message_id field from above is not present
      {
        generateUUID {
          field: id
        }
      }

      # convert the timestamp field to "yyyy-MM-dd'T'HH:mm:ss.SSSZ" format
      {
        #  21/Nov/2014:22:08:27
        convertTimestamp {
          field: time
          inputFormats: ["[dd/MMM/yyyy:HH:mm:ss", "EEE, d MMM yyyy HH:mm:ss Z", "yyyy-MM-dd'T'HH:mm:ss.SSS'Z'", "yyyy-MM-dd'T'HH:mm:ss", "yyyy-MM-dd"]
          inputTimezone: Asia/Ho_Chi_Minh
          outputFormat: "yyyy-MM-dd'T'HH:mm:ss.SSS'Z'"
          outputTimezone: UTC
        }
      }
      {
        java {
          imports: """
            import java.util.*;
          """
          code: """
            String referer = (String) record.get("referer").get(0);
            int index = referer.indexOf("://loganalyticscloudera");
            if (index != -1) {
              referer = referer.substring(0, index) + "://loganalyticscloudera/";
              record.removeAll("referer");
              record.put("referer", referer);
            }
            return child.process(record);
          """
        }
      }

      # Consume the output record of the previous command and pipe another record downstream.
      #
      # Command that sanitizes record fields that are unknown to Solr schema.xml by either
      # deleting them (renameToPrefix is absent or a zero length string), or by moving them to a
      # field prefixed with the given renameToPrefix (e.g. renameToPrefix = "ignored_" to use
      # typical dynamic Solr fields).
      #
      # Recall that Solr throws an exception on any attempt to load a document that contains a
      # field that isn't specified in schema.xml.
      {
        sanitizeUnknownSolrFields {
          # Location from which to fetch Solr schema
          solrLocator : ${SOLR_LOCATOR}

          # renameToPrefix : "ignored_"
        }
      }

      # log the record at DEBUG level to SLF4J
      { logDebug { format : "output record: {}", args : ["@{}"] } }

      # load the record into a SolrServer or MapReduce SolrOutputFormat.
      {
        loadSolr {
          solrLocator : ${SOLR_LOCATOR}
        }
      }
    ]
  }
]
